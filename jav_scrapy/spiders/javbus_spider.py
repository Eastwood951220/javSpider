import re
from typing import Optional, Dict, Any, List, Tuple
import scrapy
from scrapy.exceptions import CloseSpider
from bs4 import BeautifulSoup

from db.mongo import MongoDBManager
from logs.manager import get_logger
from task.manager import Task
from .utils import (_parse_actors,
                    _safe_extract_first,
                    _safe_join_texts,
                    _calculate_magnet_weight_javbus,
                    _parse_size,
                    _prefilter_magnets_javbus)


class JavbusSpider(scrapy.Spider):
    @property
    def logger(self):
        return self._logger

    name = "javbus_spider"
    allowed_domains = ["javbus.com"]

    custom_settings = {
        "CONCURRENT_REQUESTS": 1,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "DOWNLOAD_DELAY": 2,
        "RANDOMIZE_DOWNLOAD_DELAY": False,
        "AUTOTHROTTLE_ENABLED": False,
    }

    FIELD_MAPPING = {
        "è­˜åˆ¥ç¢¼": "code",
        "ç™¼è¡Œæ—¥æœŸ": "release_date",
        "é•·åº¦": "duration",
        "å°æ¼”": "director",
        "ç™¼è¡Œå•†": "maker",
        "ç³»åˆ—": "series",
        "é¡åˆ¥": "tags",
        "æ¼”å“¡": "actors"
    }

    def __init__(self, task: Optional[Task] = None, *args, **kwargs):
        """
        åˆå§‹åŒ–çˆ¬è™«ã€‚
        Args:
            task: Task å¯¹è±¡ï¼ŒåŒ…å« nameã€sourceã€final_urlã€keywords ç­‰ã€‚
        """
        super().__init__(**kwargs)
        self._logger = None
        self.task = task or Task(
            name="UnknownTask",
            source="javbus",
            final_url="https://javbus.com"
        )
        self.start_urls = [self.task.final_url]

        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()

        # çŠ¶æ€å˜é‡
        self.duplicate_count = 0
        self.max_duplicates = 3
        self.stop_current_actor = False

        self.total_count = 0  # æ€»å°è¯•å½±ç‰‡æ•°
        self.success_count = 0  # æˆåŠŸå½±ç‰‡æ•°
        self.fail_count = 0  # å¤±è´¥å½±ç‰‡æ•°
        self.failed_items = []
        self.chinese_count = 0  # å«ä¸­æ–‡å­—å¹•ç£åŠ›çš„å½±ç‰‡æ•°

    def _init_components(self):
        """åˆå§‹åŒ–æ—¥å¿—ã€æ•°æ®åº“ç­‰ç»„ä»¶"""
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._init_logger()

        # åˆå§‹åŒ– MongoDB
        self._init_mongodb()

    def _init_logger(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        try:
            self.logger = get_logger(
                name=f"spider_{self.task.name}",
                console=True,
                file=True,
            )
            self.logger.info(f"ğŸ•·ï¸ å¯åŠ¨ JavDB çˆ¬è™«: {self.task.name}")
            self.logger.debug(f"ä»»åŠ¡è¯¦æƒ…: {self.task}")
        except Exception as e:
            # å¦‚æœè‡ªå®šä¹‰æ—¥å¿—å¤±è´¥ï¼Œä½¿ç”¨scrapyçš„æ—¥å¿—
            self.logger = self.logger
            self.logger.error(f"âŒ è‡ªå®šä¹‰æ—¥å¿—åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ—¥å¿—: {e}")

    def _init_mongodb(self):
        """åˆå§‹åŒ– MongoDB è¿æ¥"""
        try:
            self.mongo = MongoDBManager()
            self.collection_name = self._get_collection_name()
            self.logger.info(f"âœ… å·²è¿æ¥ MongoDBï¼Œé›†åˆå: {self.collection_name}")
        except Exception as e:
            self.logger.error(f"âŒ MongoDB åˆå§‹åŒ–å¤±è´¥: {e}")
            self.mongo = None
            raise

    def _get_collection_name(self) -> str:
        """ç”Ÿæˆé›†åˆåç§°"""
        base_name = getattr(self.task, 'name', 'unknown_task').replace(" ", "_")
        return base_name

    async def start(self):
        if not self.start_urls:
            self.logger.error("âŒ æ²¡æœ‰è®¾ç½®èµ·å§‹URL")
            return

        yield scrapy.Request(
            url=self.start_urls[0],
            callback=self.parse_list,
            errback=self.handle_error,
            meta={'download_timeout': 30}
        )

    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.value}")

    def parse_list(self, response):
        self.logger.info(f"æ­£åœ¨æŠ“å–åç§°: {self.task.name}, URL: {response.url}")

        if self.stop_current_actor:
            self.logger.info(f"ğŸ›‘ å·²åœæ­¢è¯¥é¡¹ç›® {self.task.name} çš„çˆ¬å–ã€‚")
            return

        items = response.css("div.item a.movie-box")
        if not items:
            self.logger.info("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ä½œå“ï¼Œçˆ¬è™«ç»“æŸã€‚")
            return

        for item in items:
            if self.stop_current_actor:
                break
            yield from self.process_list_item(item, response)

        # å¤„ç†ä¸‹ä¸€é¡µ
        next_url = response.css("a#next::attr(href)").get()
        if next_url and not self.stop_current_actor:
            yield response.follow(
                next_url,
                callback=self.parse_list,
                errback=self.handle_error,
                priority=-10
            )

    def process_list_item(self, item, response):
        title = item.css("img::attr(title)").get() or ""
        href = item.css("::attr(href)").get()
        code = item.css("date::text").get()
        print(f"{title} | {href} | {code}")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_duplicate_item(code):
            return

        # è¯·æ±‚è¯¦æƒ…é¡µ
        yield response.follow(
            href,
            callback=self.parse_detail,
            meta={
                "name": self.task.name,
                "title": title,
                "code": code
            },
            errback=self.handle_error,
            priority=0
        )

    def _is_duplicate_item(self, code: str) -> bool:
        existing = self.mongo.find_one(self.collection_name, {
            "code": code,
            "magnet": {"$ne": None}
        })
        if existing:
            # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´
            self.mongo.update_one(self.collection_name, {"_id": existing["_id"]})

            self.duplicate_count += 1
            self.logger.info(f"â© å·²å­˜åœ¨ {code}ï¼ˆè¿ç»­é‡å¤ {self.duplicate_count}ï¼‰")

            is_skip = self.task.get("is_skip", True)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é‡å¤ä¸Šé™
            if is_skip and self.duplicate_count >= self.max_duplicates:
                self.logger.warning(f"ğŸš« ä»»åŠ¡ {self.task.name} è¿ç»­ {self.max_duplicates} ä¸ªé‡å¤ï¼Œåœæ­¢çˆ¬å–ã€‚")
                self.stop_current_actor = True
                raise CloseSpider(f"ä»»åŠ¡ {self.task.name} è¾¾åˆ°é‡å¤ä¸Šé™ï¼Œåœæ­¢")

            return True
        else:
            self.duplicate_count = 0  # é‡ç½®è®¡æ•°å™¨
            return False

    def parse_detail(self, response):
        name = response.meta["name"]
        title = response.meta["title"]
        code = response.meta["code"]
        url = response.url
        self.total_count += 1  # ğŸ¬ æ€»è®¡+1

        self.logger.info(f"æ­£åœ¨æŠ“å–åç§°: {name}-{code}-{title}")
        try:
            # è§£æåŸºæœ¬ä¿¡æ¯
            detail_data = self._parse_basic_info(response)

            script_text = "".join(response.css("script::text").getall())
            gid_match = re.search(r"var\s+gid\s*=\s*(\d+);", script_text)
            uc_match = re.search(r"var\s+uc\s*=\s*(\d+);", script_text)
            img_match = re.search(r"var\s+img\s*=\s*['\"](.+?)['\"];", script_text)

            if not gid_match or not uc_match or not img_match:
                self.fail_count += 1  # âŒ æ— ç£åŠ›é“¾æ¥
                self.logger.warning(f"âš ï¸ æ— æ³•è§£æ gid/uc/img: {response.url}")
                return

            gid = gid_match.group(1)
            uc = uc_match.group(1)
            img = img_match.group(1)

            ajax_url = f"https://www.javbus.com/ajax/uncledatoolsbyajax.php?gid={gid}&lang=zh&img={img}&uc={uc}&floor=735"

            yield scrapy.Request(
                url=ajax_url,
                callback=self.parse_magnet_ajax,
                meta={
                    "title": title,
                    "code": code,
                    "detail_data": detail_data
                },
                errback=self.handle_error,
                priority=10
            )
        except Exception as e:
            self.fail_count += 1
            self.failed_items.append({
                "task": self.task.name,
                "code": code,
                "url": url,
                "reason": f"è§£æå‡ºé”™: {e}"
            })
            self.logger.error(f"âŒ è§£æè¯¦æƒ…é¡µå¤±è´¥: {code} | é”™è¯¯: {e}")

    def _parse_basic_info(self, response) -> Dict[str, Any]:
        """
        è§£æ JavBus å½±ç‰‡è¯¦æƒ…é¡µåŸºæœ¬ä¿¡æ¯

        Args:
            response: scrapy Response å¯¹è±¡
        Returns:
            Dict[str, Any]: åŒ…å«å½±ç‰‡åŸºæœ¬ä¿¡æ¯çš„å­—å…¸
        """

        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result: Dict[str, Any] = {
            "code": "",
            "title": "",
            "release_date": "",
            "duration": 0,
            "director": "",
            "maker": "",
            "series": "",
            "tags": [],
            "actors": []
        }

        try:
            info_block = response.css("div.row.movie div.info")

            # å°é¢ä¸æ ‡é¢˜
            result["title"] = _safe_extract_first(response.css(".screencap img::attr(title)"))

            # æå–ä¸»è¦ä¿¡æ¯
            for p in info_block.css("p"):
                header = _safe_extract_first(p.css(".header::text")).rstrip(":ï¼š")

                if not header or header not in self.FIELD_MAPPING:
                    continue

                key = self.FIELD_MAPPING[header]
                value = self._extract_field_value(header, p)

                # ç±»å‹å…¼å®¹ï¼šé˜²æ­¢è¿”å› None æˆ–å¼‚å¸¸ç±»å‹
                if isinstance(result[key], list) and isinstance(value, list):
                    result[key].extend(value)
                else:
                    result[key] = value

            # å¦‚æœæ¼”å‘˜ä¸ºç©ºï¼Œå†åšå…œåº•è§£æ
            if not result["actors"]:
                result["actors"] = _parse_actors(response)

        except Exception as e:
            self.logger.warning(f"[è§£æå¤±è´¥] {response.url} ({type(e).__name__}): {e}")

        return result

    def _extract_field_value(self, header: str, p_element) -> Any:
        """æ ¹æ®å­—æ®µå¤´æå–å¯¹åº”çš„å€¼"""
        try:
            if header == "ç™¼è¡Œæ—¥æœŸ":
                # æå–æ‰€æœ‰æ–‡æœ¬å¹¶å»æ‰å­—æ®µå¤´
                text = _safe_join_texts(p_element)
                text = re.sub(r"[ç™¼è¡Œæ—¥æœŸ:ï¼š\s]", "", text)
                # å…¼å®¹å¤šç§æ—¥æœŸæ ¼å¼
                match = re.search(r"\d{4}[-/]\d{2}[-/]\d{2}", text)
                return match.group() if match else ""

            elif header == "é•·åº¦":
                text = _safe_join_texts(p_element)
                match = re.search(r"\d+", text)
                return int(match.group()) if match else 0

            elif header in ["å°æ¼”", "ç™¼è¡Œå•†", "ç³»åˆ—"]:
                return _safe_extract_first(p_element.css("a::text"))

            elif header == "é¡åˆ¥":
                # å‘ä¸‹æŸ¥æ‰¾ä¸‹ä¸€ä¸ª <p> æ ‡ç­¾ï¼ˆå…„å¼ŸèŠ‚ç‚¹ï¼‰
                next_p = p_element.xpath("following-sibling::p[1]")
                return [
                    t.strip() for t in next_p.css(".genre a::text").getall() if t.strip()
                ]

            elif header == "æ¼”å“¡":
                return [t.strip() for t in p_element.css("a::text").getall() if t.strip()]

            elif header == "è­˜åˆ¥ç¢¼":
                # åªé€‰ç¬¬äºŒä¸ª spanï¼ˆæ’é™¤ headerï¼‰
                spans = p_element.css("span::text").getall()
                if len(spans) >= 2:
                    return spans[1].strip()
                # æˆ–ç›´æ¥é€‰æ‹©é .header çš„ span
                code = p_element.css("span:not(.header)::text").get()
                return code.strip() if code else ""

        except Exception as e:
            self.logger.debug(f"å­—æ®µè§£æå¼‚å¸¸ [{header}]: {e}")

        # é»˜è®¤è¿”å›
        return [] if header in ["é¡åˆ¥", "æ¼”å“¡"] else ""

    # ----------------------------------------------------------------------

    def parse_magnet_ajax(self, response):
        code = response.meta["code"]
        title = response.meta["title"]
        detail_data = response.meta["detail_data"]
        self.logger.info(f"ajaxè¯·æ±‚ {code} | {title}")

        # # è·å–æœ€ä½³ç£åŠ›é“¾æ¥
        best_magnet, max_size, has_chinese_sub = self.get_best_magnet(
            response,
            only_chinese=self.task.filter.get("only_chinese", False)
        )
        if not best_magnet:
            self.fail_count += 1  # âŒ æ— ç£åŠ›é“¾æ¥
            self.failed_items.append({
                "task": self.task.name,
                "code": code,
                "reason": "æ— å¯ç”¨ç£åŠ›é“¾æ¥"
            })
            self.logger.info(f"âš ï¸ {code} | {title} æ²¡æœ‰å¯ç”¨ç£åŠ›é“¾æ¥ï¼Œè·³è¿‡")
            return

        # æ„å»ºæœ€ç»ˆæ•°æ®
        item = self._build_final_item(
            title, code, best_magnet, max_size, has_chinese_sub, detail_data
        )
        self.success_count += 1  # âœ… æˆåŠŸå½±ç‰‡ +1
        if has_chinese_sub:
            self.chinese_count += 1  # ğŸ‡¨ğŸ‡³ å«ä¸­æ–‡å­—å¹• +1
        self.logger.info(f"âœ… å®Œæ•´è¯¦æƒ…: {code} | {title} |  å¤§å°: {max_size}MB")

        yield item

    def get_best_magnet(self, response, only_chinese: bool = False) -> Tuple[str, float, bool]:
        """
        ä» JavBus è¯¦æƒ…é¡µæå–æœ€ä½³ç£åŠ›é“¾æ¥

        Args:
            response: scrapy Response å¯¹è±¡
            only_chinese: æ˜¯å¦ä»…é€‰æ‹©ä¸­æ–‡å­—å¹•ç‰ˆæœ¬

        Returns:
            (best_magnet_url, weight, has_chinese_sub)
        """
        soup = BeautifulSoup(response.text, "lxml")
        magnets = soup.find_all("tr")
        if not magnets:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç£åŠ›é“¾æ¥: {response.url}")
            return "", 0.0, False

        # é¢„è¿‡æ»¤
        filtered_magnets = _prefilter_magnets_javbus(magnets, only_chinese)
        best_magnet = ""
        max_weight = 0
        has_chinese_sub = False

        for magnet_elem in filtered_magnets:
            magnet_data = self._parse_magnet_element(magnet_elem)

            if not magnet_data:
                continue

            weight = _calculate_magnet_weight_javbus(magnet_data)

            if weight >= max_weight:
                best_magnet = magnet_data["url"]
                max_weight = weight
                has_chinese_sub = magnet_data["has_chinese_sub"]

        return best_magnet, round(max_weight, 2), has_chinese_sub

    def _parse_magnet_element(self, magnet_elem) -> Optional[Dict]:
        """è§£æå•ä¸ªç£åŠ›é“¾æ¥å…ƒç´ """
        try:

            # æå– magnet é“¾æ¥
            a_tag = magnet_elem.find("a", href=True)
            if not a_tag:
                return None

            magnet_url = a_tag["href"].strip()
            if not magnet_url.startswith("magnet:?"):
                return None

            # æ–‡ä»¶å¤§å°ï¼ˆç¬¬äºŒåˆ—ï¼‰
            size_td = magnet_elem.find_all("td")[1] if len(magnet_elem.find_all("td")) > 1 else None
            size_text = size_td.get_text(strip=True) if size_td else ""
            size = _parse_size(size_text)

            # æ—¥æœŸï¼ˆç¬¬ä¸‰åˆ—ï¼‰
            date_td = magnet_elem.find_all("td")[2] if len(magnet_elem.find_all("td")) > 2 else None
            date = date_td.get_text(strip=True) if date_td else ""

            # æ ‡ç­¾ï¼ˆç¬¬ä¸€åˆ—å†…çš„ .btnï¼‰
            tags = [btn.get_text(strip=True) for btn in magnet_elem.select(".btn")]
            has_chinese_sub = any("ä¸­å­—" in t or "å­—å¹•" in t for t in tags)

            return {
                "url": magnet_url,
                "size": size,
                "tags": tags,
                "has_chinese_sub": has_chinese_sub
            }

        except Exception as e:
            self.logger.warning(f"âŒ è§£æç£é“¾å‡ºé”™: {e}")
            return None

    # ----------------------------------------------------------------------

    def _build_final_item(self, title: str, code: str, best_magnet: str,
                          max_size: float, has_chinese_sub: bool,
                          detail_data: Dict) -> Dict[str, Any]:
        """æ„å»ºæœ€ç»ˆçš„æ•°æ®é¡¹"""
        tags = detail_data.get("tags", []).copy()
        if has_chinese_sub and "å­—å¹•" not in tags:
            tags.append("ä¸­æ–‡å­—å¹•")

        return {
            "name": self.task.name,
            "title": title,
            "code": code,
            "magnet": best_magnet,
            "size": max_size,
            "release_date": detail_data["release_date"],
            "director": detail_data["director"],
            "maker": detail_data["maker"],
            "series": detail_data["series"],
            "tags": tags,
            "actors": detail_data["actors"],
        }

    @logger.setter
    def logger(self, value):
        self._logger = value

    def close(self, reason):
        """çˆ¬è™«ç»“æŸæ—¶è¾“å‡ºç»Ÿè®¡ç»“æœ"""
        self.logger.info("ğŸ“Š çˆ¬å–ç»Ÿè®¡ç»“æœ --------------------------------")
        self.logger.info(f"ğŸ§© ä»»åŠ¡åç§°: {self.task.name}")
        self.logger.info(f"ğŸ¬ æ€»è®¡å°è¯•å½±ç‰‡æ•°: {self.total_count}")
        self.logger.info(f"âœ… æˆåŠŸè·å–ç£åŠ›æ•°: {self.success_count}")
        self.logger.info(f"ğŸ‡¨ğŸ‡³ å«ä¸­æ–‡å­—å¹•ç£åŠ›æ•°: {self.chinese_count}")
        self.logger.info(f"âŒ å¤±è´¥ï¼ˆæ— ç£åŠ›/å‡ºé”™ï¼‰æ•°: {self.fail_count}")
        self.logger.info(f"ğŸ“¦ çˆ¬è™«ç»“æŸåŸå› : {reason}")
        self.logger.info("--------------------------------------------")

        if self.failed_items:
            self.logger.warning("â— ä»¥ä¸‹å½±ç‰‡æœªæˆåŠŸçˆ¬å–:")
            for fail in self.failed_items:
                self.logger.warning(
                    f"  [ä»»åŠ¡: {fail['task']}] {fail['code']} | {fail['reason']} | URL: {fail['url']}"
                )