#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
app/main.py
ä»»åŠ¡å¯åŠ¨å…¥å£ï¼šè‡ªåŠ¨åŠ è½½ task.yml ä¸­çš„ä»»åŠ¡å¹¶é¡ºåºè¿è¡Œå¯¹åº” Scrapy çˆ¬è™«
"""

# -------------------------------------------------------
# ğŸš¨ å¿…é¡»åœ¨å¯¼å…¥ Scrapy å‰å®‰è£…æ­£ç¡®çš„ reactorï¼
# -------------------------------------------------------
from twisted.internet import asyncioreactor
asyncioreactor.install()

import logging
import sys
import signal
import time
from pathlib import Path
from typing import List, Optional, Type

from twisted.internet import reactor, defer
from twisted.internet.task import deferLater
from scrapy.crawler import CrawlerRunner
from scrapy.utils.project import get_project_settings

# -------------------------------------------------------
# ğŸ§  è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
# -------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from logs.manager import get_logger
from task.manager import TaskLoader, Task

# -------------------------------------------------------
# ğŸ”— source ä¸çˆ¬è™«ç±»æ˜ å°„
# -------------------------------------------------------
SPIDER_MAP = {
    "javdb": "jav_scrapy.spiders.javdb_spider.JavdbSpider",
    "javbus": "jav_scrapy.spiders.javbus_spider.JavbusSpider",
}


def load_spider_class(source: str) -> Type:
    """åŠ¨æ€åŠ è½½çˆ¬è™«ç±»"""
    spider_path = SPIDER_MAP.get(source.lower())
    if not spider_path:
        raise ValueError(f"æœªæ‰¾åˆ°ä¸ source='{source}' å¯¹åº”çš„çˆ¬è™«ç±»æ˜ å°„")

    try:
        module_name, class_name = spider_path.rsplit(".", 1)
        module = __import__(module_name, fromlist=[class_name])
        return getattr(module, class_name)
    except (ImportError, AttributeError) as e:
        raise ImportError(f"åŠ è½½çˆ¬è™«ç±»å¤±è´¥: {spider_path} - {e}") from e


class TaskRunner:
    """ğŸ•¹ï¸ é¡ºåºæ‰§è¡Œ Scrapy çˆ¬è™«ä»»åŠ¡"""

    def __init__(self) -> None:
        self.logger = get_logger("task_runner", console=True, file=True, level=logging.INFO)
        self.runner: Optional[CrawlerRunner] = None
        self.start_time: Optional[float] = None
        self._register_signal_handlers()

    # ------------------------------------------------------------------
    # ğŸ“¡ ç³»ç»Ÿä¿¡å·ä¸åˆå§‹åŒ–
    # ------------------------------------------------------------------
    def _register_signal_handlers(self) -> None:
        """æ³¨å†Œé€€å‡ºä¿¡å·ä»¥æ”¯æŒä¼˜é›…å…³é—­"""
        def handle_exit(signum, frame):
            self.logger.warning(f"âš ï¸ æ•è·ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
            if reactor.running:
                reactor.stop()
            sys.exit(0)

        signal.signal(signal.SIGINT, handle_exit)   # Ctrl + C
        signal.signal(signal.SIGTERM, handle_exit)  # kill

    # ------------------------------------------------------------------
    # ğŸ§© ä»»åŠ¡å¤„ç†é€»è¾‘
    # ------------------------------------------------------------------

    def validate_tasks(self, tasks: List[Task]) -> List[Task]:
        """éªŒè¯å¹¶è¿‡æ»¤æ— æ•ˆä»»åŠ¡"""
        valid = []
        for task in tasks:
            if not task.name or not task.source:
                self.logger.warning(f"è·³è¿‡æ— æ•ˆä»»åŠ¡: ç¼ºå°‘å¿…è¦å­—æ®µ - {task}")
                continue

            if task.source.lower() not in SPIDER_MAP:
                self.logger.warning(f"è·³è¿‡ä»»åŠ¡ '{task.name}': ä¸æ”¯æŒçš„ source '{task.source}'")
                continue

            if not task.final_url or not task.final_url.startswith(("http://", "https://")):
                self.logger.warning(f"è·³è¿‡ä»»åŠ¡ '{task.name}': æ— æ•ˆçš„ URL '{task.final_url}'")
                continue

            valid.append(task)

        self.logger.info(f"âœ… ä»»åŠ¡éªŒè¯å®Œæˆ: {len(valid)}/{len(tasks)} ä¸ªæœ‰æ•ˆä»»åŠ¡")
        return valid

    # ------------------------------------------------------------------
    # ğŸª¶ æ—¥å¿—è¾“å‡ºé€»è¾‘
    # ------------------------------------------------------------------
    def _log_task_start(self, task: Task, index: int, total: int):
        self.logger.info("-" * 60)
        self.logger.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡ [{index}/{total}]: {task.name} ({task.source})")
        self.logger.debug(f"ğŸŒ ç›®æ ‡: {task.final_url}")

    def _log_task_result(self, task: Task, success: bool, duration: float):
        icon = "âœ…" if success else "âŒ"
        status = "å®Œæˆ" if success else "å¤±è´¥"
        self.logger.info(f"{icon} ä»»åŠ¡{status}: {task.name} | è€—æ—¶ {duration:.2f}s")

    def _log_final_summary(self, duration: float, success: int, failed: int):
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•")
        self.logger.info(f"âœ… æˆåŠŸä»»åŠ¡æ•°: {success}")
        self.logger.info(f"âŒ å¤±è´¥ä»»åŠ¡æ•°: {failed}")
        self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
        self.logger.info("=" * 60)

    def _display_tasks_summary(self, tasks: List[Task]):
        """æ‰“å°ä»»åŠ¡æ‘˜è¦"""
        self.logger.info("ğŸ“‹ å³å°†æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š")
        self.logger.info("=" * 60)
        for i, task in enumerate(tasks, 1):
            self.logger.info(f"{i}. {task.name} [{task.source}] â†’ {task.final_url}")
        self.logger.info("=" * 60)

    # ------------------------------------------------------------------
    # ğŸ•·ï¸ çˆ¬è™«æ‰§è¡Œé€»è¾‘
    # ------------------------------------------------------------------
    @defer.inlineCallbacks
    def run_spiders(self, tasks: List[Task]):
        """æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡"""
        self.start_time = time.time()
        settings = get_project_settings()
        self.runner = CrawlerRunner(settings)

        success, failed = 0, 0
        total = len(tasks)
        self.logger.info(f"ğŸ¯ å…± {total} ä¸ªä»»åŠ¡ï¼Œå¼€å§‹é¡ºåºæ‰§è¡Œ...")

        for idx, task in enumerate(tasks, start=1):
            start = time.time()
            try:
                self._log_task_start(task, idx, total)
                spider_cls = load_spider_class(task.source)
                yield self.runner.crawl(spider_cls, task=task)
                self._log_task_result(task, True, time.time() - start)
                success += 1

                # ä»»åŠ¡é—´å»¶è¿Ÿï¼ˆé»˜è®¤ 2 ç§’ï¼‰
                if idx < total:
                    delay = getattr(task, "delay", 2)
                    if delay > 0:
                        self.logger.debug(f"â³ ç­‰å¾… {delay} ç§’åæ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡...")
                        yield deferLater(reactor, delay, lambda: None)

            except Exception as e:
                failed += 1
                self.logger.exception(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.name} - {e}")
                self._log_task_result(task, False, time.time() - start)
                self.logger.info("â© ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡...")
                continue

        self._log_final_summary(time.time() - self.start_time, success, failed)
        reactor.stop()

    # ------------------------------------------------------------------
    # ğŸ§  ä¸»è°ƒåº¦å…¥å£
    # ------------------------------------------------------------------
    def main(self):
        """ç¨‹åºä¸»å…¥å£"""
        self.logger.info("ğŸš€ å¯åŠ¨ä»»åŠ¡è°ƒåº¦ç¨‹åº...")

        try:
            loader = TaskLoader()
            tasks = loader.get_all_tasks()
            if not tasks:
                self.logger.warning("âš ï¸ æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»åŠ¡")
                return

            valid_tasks = self.validate_tasks(tasks)
            if not valid_tasks:
                self.logger.error("âŒ æ— æœ‰æ•ˆä»»åŠ¡å¯æ‰§è¡Œ")
                return

            self._display_tasks_summary(valid_tasks)
            self.run_spiders(valid_tasks)
            reactor.run()

        except Exception as e:
            self.logger.exception(f"ğŸ’¥ ç¨‹åºè¿è¡Œå¼‚å¸¸: {e}")
            if reactor.running:
                reactor.stop()
            sys.exit(1)


def main():
    """CLI å…¥å£"""
    TaskRunner().main()

if __name__ == "__main__":
    main()
