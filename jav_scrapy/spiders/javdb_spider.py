#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
jav_scrapy/spiders/javdb_spider.py
JavDB çˆ¬è™«ï¼šæ ¹æ® task åŠ¨æ€é…ç½®æ—¥å¿—ä¸ MongoDBã€‚
"""
from typing import Optional, Dict, Any, Tuple
import scrapy
from scrapy.exceptions import CloseSpider

from logs.manager import get_logger
from cookies.manager import CookieManager
from task.manager import Task
from db.mongo import MongoDBManager
from .utils import (_parse_field_value,
                    _should_skip_item,
                    _calculate_magnet_weight,
                    _prefilter_magnets,
                    _parse_size)


class JavdbSpider(scrapy.Spider):
    @property
    def logger(self):
        return self._logger

    name = "javdb_spider"
    allowed_domains = ["javdb.com"]

    # å­—æ®µæ˜ å°„è¡¨ï¼Œé¿å…é‡å¤çš„if-elseåˆ¤æ–­
    FIELD_MAPPING = {
        "æ—¥æœŸ": "release_date",
        "æ™‚é•·": "duration",
        "å°æ¼”": "director",
        "ç‰‡å•†": "maker",
        "ç³»åˆ—": "series",
        "è©•åˆ†": "rating",
        "é¡åˆ¥": "tags",
        "æ¼”å“¡": "actors"
    }

    def __init__(self, task: Optional[Task] = None, *args, **kwargs):
        """
        åˆå§‹åŒ–çˆ¬è™«ã€‚
        Args:
            task: Task å¯¹è±¡ï¼ŒåŒ…å« nameã€sourceã€final_urlã€keywords ç­‰ã€‚
        """
        super().__init__(**kwargs)
        self._logger = None
        self.task = task or Task(
            name="UnknownTask",
            source="javdb",
            final_url="https://javdb.com"
        )
        self.start_urls = [self.task.final_url]

        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()

        # çŠ¶æ€å˜é‡
        self.duplicate_count = 0
        self.max_duplicates = 5
        self.stop_current_actor = False

    def _init_components(self):
        """åˆå§‹åŒ–æ—¥å¿—ã€æ•°æ®åº“ç­‰ç»„ä»¶"""
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._init_logger()

        # åˆå§‹åŒ– MongoDB
        self._init_mongodb()

        # åˆå§‹åŒ– Cookies
        self._init_cookies()

    def _init_logger(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        try:
            self.logger = get_logger(
                name=f"spider_{self.task.name}",
                console=True,
                file=True,
            )
            self.logger.info(f"ğŸ•·ï¸ å¯åŠ¨ JavDB çˆ¬è™«: {self.task.name}")
            self.logger.debug(f"ä»»åŠ¡è¯¦æƒ…: {self.task}")
        except Exception as e:
            # å¦‚æœè‡ªå®šä¹‰æ—¥å¿—å¤±è´¥ï¼Œä½¿ç”¨scrapyçš„æ—¥å¿—
            self.logger = self.logger
            self.logger.error(f"âŒ è‡ªå®šä¹‰æ—¥å¿—åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ—¥å¿—: {e}")

    def _init_mongodb(self):
        """åˆå§‹åŒ– MongoDB è¿æ¥"""
        try:
            self.mongo = MongoDBManager()
            self.collection_name = self._get_collection_name()
            self.logger.info(f"âœ… å·²è¿æ¥ MongoDBï¼Œé›†åˆå: {self.collection_name}")
        except Exception as e:
            self.logger.error(f"âŒ MongoDB åˆå§‹åŒ–å¤±è´¥: {e}")
            self.mongo = None
            raise

    def _init_cookies(self):
        """åˆå§‹åŒ– Cookies"""
        try:
            cookie_mgr = CookieManager()
            self.cookies = cookie_mgr.load_cookies('javdb')
            self.logger.info("âœ… å·²åŠ è½½ Cookies")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Cookies åŠ è½½å¤±è´¥: {e}")
            self.cookies = None

    def _get_collection_name(self) -> str:
        """ç”Ÿæˆé›†åˆåç§°"""
        base_name = getattr(self.task, 'name', 'unknown_task').replace(" ", "_")
        return base_name

    async def start(self):
        if not self.start_urls:
            self.logger.error("âŒ æ²¡æœ‰è®¾ç½®èµ·å§‹URL")
            return

        yield scrapy.Request(
            url=self.start_urls[0],
            cookies=self.cookies,
            callback=self.parse_list,
            errback=self.handle_error,
            meta={'download_timeout': 30}
        )

    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.value}")

    def parse_list(self, response):
        self.logger.info(f"æ­£åœ¨æŠ“å–åç§°: {self.task.name}, URL: {response.url}")

        if self.stop_current_actor:
            self.logger.info(f"ğŸ›‘ å·²åœæ­¢è¯¥é¡¹ç›® {self.task.name} çš„çˆ¬å–ã€‚")
            return

        items = response.css("div.item a.box")
        if not items:
            self.logger.info("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ä½œå“ï¼Œçˆ¬è™«ç»“æŸã€‚")
            return

        for item in items:
            if self.stop_current_actor:
                break
            req = self.process_list_item(item, response)
            if req:
                req.priority = 10  # è¯¦æƒ…é¡µä¼˜å…ˆçº§é«˜
                yield req

        # å¤„ç†ä¸‹ä¸€é¡µ
        next_url = response.css("nav.pagination a[rel='next']::attr(href)").get()
        if next_url and not self.stop_current_actor:
            yield response.follow(
                next_url,
                callback=self.parse_list,
                errback=self.handle_error,
                priority=-10
            )

    def process_list_item(self, item, response):
        title = item.css("::attr(title)").get() or ""
        href = response.urljoin(item.css("::attr(href)").get())
        code = item.css(".video-title strong::text").get() or ""

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_duplicate_item(code):
            return

            # è¯·æ±‚è¯¦æƒ…é¡µ
        yield response.follow(
            href,
            callback=self.parse_detail,
            meta={
                "name": self.task.name,
                "title": title,
                "code": code
            },
            errback=self.handle_error
        )

    def _is_duplicate_item(self, code: str) -> bool:
        existing = self.mongo.find_one(self.collection_name, {
            "code": code,
            "magnet": {"$ne": None}
        })
        if existing:
            # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´
            self.mongo.update_one(self.collection_name, {"_id": existing["_id"]})

            self.duplicate_count += 1
            self.logger.info(f"â© å·²å­˜åœ¨ {code}ï¼ˆè¿ç»­é‡å¤ {self.duplicate_count}ï¼‰")

            is_skip = self.task.get("is_skip", True)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é‡å¤ä¸Šé™
            if is_skip and self.duplicate_count >= self.max_duplicates:
                self.logger.warning(f"ğŸš« ä»»åŠ¡ {self.task.name} è¿ç»­ {self.max_duplicates} ä¸ªé‡å¤ï¼Œåœæ­¢çˆ¬å–ã€‚")
                self.stop_current_actor = True
                raise CloseSpider(f"ä»»åŠ¡ {self.task.name} è¾¾åˆ°é‡å¤ä¸Šé™ï¼Œåœæ­¢")

            return True
        else:
            self.duplicate_count = 0  # é‡ç½®è®¡æ•°å™¨
            return False

    def parse_detail(self, response):
        name = response.meta["name"]
        title = response.meta["title"]
        code = response.meta["code"]

        self.logger.info(f"æ­£åœ¨æŠ“å–åç§°: {name}-{code}-{title}")
        # è§£æåŸºæœ¬ä¿¡æ¯
        detail_data = self._parse_basic_info(response)

        # åº”ç”¨è¿‡æ»¤å™¨
        # if _should_skip_item(self.task.filter, detail_data):
        #     self.logger.info(f"â© {title} | {code} è¢«è¿‡æ»¤å™¨è·³è¿‡")
        #     return

        # è·å–æœ€ä½³ç£åŠ›é“¾æ¥
        best_magnet, max_size, has_chinese_sub = self.get_best_magnet(
            response,
            only_chinese=self.task.filter.get("only_chinese", False)
        )

        if not best_magnet:
            self.logger.info(f"âš ï¸ {code} | {title} æ²¡æœ‰å¯ç”¨ç£åŠ›é“¾æ¥ï¼Œè·³è¿‡")
            return

        # æ„å»ºæœ€ç»ˆæ•°æ®
        item = self._build_final_item(
            title, code, best_magnet, max_size, has_chinese_sub, detail_data
        )

        self.logger.info(f"âœ… å®Œæ•´è¯¦æƒ…: {code} | {title} |  å¤§å°: {max_size}MB")
        yield item

    def _parse_basic_info(self, response) -> Dict[str, Any]:
        """è§£æåŸºæœ¬ä¿¡æ¯é¢æ¿"""
        panel_blocks = response.css("nav.movie-panel-info > div.panel-block")

        # åˆå§‹åŒ–é»˜è®¤å€¼
        result = {
            "release_date": "",
            "duration": 0,
            "director": "",
            "maker": "",
            "series": "",
            "rating": 0.0,
            "tags": [],
            "actors": []
        }

        for block in panel_blocks:
            field_name = block.css("strong::text").get(default="").strip().rstrip(":")
            value_span = block.css("span.value")

            if field_name not in self.FIELD_MAPPING:
                continue

            field_key = self.FIELD_MAPPING[field_name]
            result[field_key] = _parse_field_value(field_name, value_span)

        return result

    def get_best_magnet(self, response, only_chinese: bool = False) -> Tuple[str, float, bool]:
        """
        ä» JavDB è¯¦æƒ…é¡µæå–æœ€ä½³ç£åŠ›é“¾æ¥
        :return: tuple(best_magnet, max_size_in_MB, has_chinese_sub)
        """
        magnets = response.css("#magnets-content .item")

        # é¢„è¿‡æ»¤
        filtered_magnets = _prefilter_magnets(magnets, only_chinese)

        best_magnet = ""
        max_weight = 0
        has_chinese_sub = False

        for magnet_elem in filtered_magnets:
            magnet_data = self._parse_magnet_element(magnet_elem)
            if not magnet_data:
                continue

            weight = _calculate_magnet_weight(magnet_data)

            if weight > max_weight:
                best_magnet = magnet_data["url"]
                max_weight = weight
                has_chinese_sub = magnet_data["has_chinese_sub"]

        return best_magnet, round(max_weight, 2), has_chinese_sub

    def _parse_magnet_element(self, magnet_elem) -> Optional[Dict]:
        """è§£æå•ä¸ªç£åŠ›é“¾æ¥å…ƒç´ """
        try:
            # è·å–ç£åŠ›é“¾æ¥
            magnet_url = (magnet_elem.css("button.copy-to-clipboard::attr(data-clipboard-text)").get() or
                          magnet_elem.css(".magnet-name a::attr(href)").get())

            if not magnet_url or not magnet_url.startswith("magnet:?"):
                return None

            # è·å–æ–‡ä»¶å¤§å°
            meta_text = magnet_elem.css(".magnet-name .meta::text").get(default="").strip()
            size = _parse_size(meta_text)

            # è·å–æ ‡ç­¾
            tags = magnet_elem.css(".magnet-name .tags .tag::text").getall()
            has_chinese_sub = any("å­—å¹•" in tag for tag in tags)

            return {
                "url": magnet_url,
                "size": size,
                "tags": tags,
                "meta_text": meta_text,
                "has_chinese_sub": has_chinese_sub
            }

        except Exception as e:
            self.logger.warning(f"âŒ è§£æç£é“¾å‡ºé”™: {e}")
            return None

    def _build_final_item(self, title: str, code: str, best_magnet: str,
                          max_size: float, has_chinese_sub: bool,
                          detail_data: Dict) -> Dict[str, Any]:
        """æ„å»ºæœ€ç»ˆçš„æ•°æ®é¡¹"""
        tags = detail_data["tags"].copy()
        if has_chinese_sub and "å­—å¹•" not in tags:
            tags.append("ä¸­æ–‡å­—å¹•")

        return {
            "name": self.task.name,
            "title": title,
            "code": code,
            "magnet": best_magnet,
            "size": max_size,
            "release_date": detail_data["release_date"],
            "director": detail_data["director"],
            "maker": detail_data["maker"],
            "series": detail_data["series"],
            "rating": detail_data["rating"],
            "tags": tags,
            "actors": detail_data["actors"],
        }

    @logger.setter
    def logger(self, value):
        self._logger = value
